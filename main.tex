\documentclass[12pt,a4paper]{article}

%---------------------------------
% PACKAGES
%---------------------------------
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

%---------------------------------
% THEOREM ENVIRONMENTS
%---------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

%---------------------------------
% TITLE INFORMATION
%---------------------------------
\title{ \textbf{Physics Concepts and Fundamentals}\\[4pt]
\large Generic introduction to some concepts in physics}

%---------------------------------
% DOCUMENT
%---------------------------------
\begin{document}

\maketitle
\tableofcontents
\newpage

%---------------------------------
\section{Generic Mathematical Tools}
\subsection{Legendre Transform}
Method to rewrite a convex function as a different one replacing one variable with the derivative of the original function:
\begin{equation}
	\mathcal{L}_T(f(\vec{x}))(\vec{p}) = \sup_{\vec{x}\in \mathbb{R}^N} \left[ \langle \vec{p},\vec{x} \rangle - f(\vec{x}) \right] \;,
\end{equation}
Where it emerges that the $\sup$ is obtained exactly at a certain $\vec{x}^*=\nabla f(\vec{x}^*)\equiv \vec{p}.$

This transform is crucial in Hamiltonian mechanics to switch between a second order differential equation, to a system of first order differential equations (Hamilton equations).

\subsection{Differentials}
Let $f(\vec{x})$ be a multivariate function. The differential of $f$ can be defined as the overall infinitesimal displacement of $f$ in any direction:
\begin{equation}
	df(\vec{x}) = \dfrac{\partial f}{\partial x_1}dx_1 + \dfrac{\partial f}{\partial x_2}dx_2 + \dots + \dfrac{\partial f}{\partial x_n}dx_n \; .
\end{equation}
The differential is a linear operator in the dual space of the tangent plane of $f$ in $\vec{x}$ on the manifold $M$ where the function lives:
\begin{align*}
	df_{\vec{x}} & :\; T_{\vec{x}} M \rightarrow \mathbb{R}\;, \; df_{\vec{x}}\in T^*_{\vec{x}} M \\[2pt]
	&\vec{v} \mapsto df_{\vec{x}}(\vec{v}) = \sum_{i=1}^{n}\dfrac{\partial f}{\partial x_i}\bigg|_{\vec{x}} v_i
\end{align*}

\subsection{Integrals}
Generalization of an infinite sum of a function over an interval divided in infinitesimal steps equally spaced. We start by considering the finite steps $\Delta x$
..

Complex integral
...

\section{Newtonian Mechanics}
Solve the dynamics of a system by modeling it to a vectorial problem and applying the principles in order to get a second order differential equation.
\begin{itemize}
	\item $\vec{r}(t)$ : position $\longrightarrow \dfrac{d}{dt} (\vec{r}(t)) = \vec{v}(t) \equiv \dot{\vec{x}}(t)\;$ : velocity
	\item $\vec{v}(t)$ : velocity $\longrightarrow \dfrac{d}{dt} (\vec{v}(t)) = \vec{a}(t) \equiv \dot{\vec{v}}(t) = \ddot{\vec{x}}(t) \;$ : acceleration
\end{itemize}
The second principle of Newtonian mechanics is:
\begin{equation}
	\vec{F}(t) = m\vec{a}(t)
\end{equation}
therefore we set the differential system:
\begin{align*}
&\begin{cases}
	\vec{F}(t) = m\vec{a}(t) = 	m \frac{d}{dt}(v(t))\\
	\vec{v}(t) = \frac{d}{dt}(r(t))\\
	\vec{v}(0) = \vec{v}_0 \\
	\vec{r}(0) = \vec{r}_0
\end{cases}\\
&\Rightarrow m\dfrac{d^2}{dt^2}(\vec{r}(t)) = F(\vec{r},t)
\end{align*}
Hooke example..

\section{Lagrangian Mechanics}
Solve the dynamics of a system using only algebraic procedures to get the same second order differential equation system coming from Newtonian mechanics -- but it is general even to non relativistic mechanics.
\subsection{Mass Matrix}
Before setting the Lagrangian system, we need to define the \textit{mass matrix} so that we can generalize the concept of \textbf{kinetic energy}.
We define the kinetic energy as a scalar quantity in the $i$-th direction:
\begin{equation*}
	dE_i = dr_i \, F_i = dr_i \, m_i \, \ddot{r}_i = \frac{dr_i}{dt}\, m_i \, d\dot{r}_i \;,
\end{equation*}
that we can rewrite using $d(\dot{r}_i\cdot \dot{r}_i) = 2\dot{r}_i\,dr_i$ as:
\begin{align*}
	&dE_i = m_i \, \frac{1}{2}d(\dot{r}_i^2)\\
	&\Rightarrow E_i = \int_{\dot{r}_0}^{\dot{r}_1}\frac{1}{2} m_i d(\dot{r}_i^2) = \frac{1}{2} m_i (\dot{r}_{i,1}^2 - \dot{r}_{i,0}^2) = \frac{1}{2} m_i \dot{r}_i^2
\end{align*}
We can write all the contributions to the energy in a vectorial form:
\begin{equation*}
E = \frac{1}{2} \, \dot{\vec{r}}^\top M(\vec{r},t) \, \dot{\vec{r}}\; ,
\end{equation*}
with $M(\vec{r},t)$ being the \textbf{mass matrix}. In the euclidean coordinates, $M(\vec{r},t)\equiv M_{eu}$ is diagonal filled with the single masses -- the difficulty is in defining the velocities. Using the generalized coordinates instead $\vec{r}=\vec{r}(\vec{q})$ we get:
\begin{equation*}
\dfrac{d\vec{r}\, (\vec{q})}{dt} = \sum_{i} \dfrac{\partial \vec{r}_k}{\partial q_i} \, \dot{q}_i + \dfrac{\partial \vec{r}(\vec{q})}{\partial t} = J_q\dot{\vec{q}} + \dfrac{\partial \vec{r}(\vec{q})}{\partial t} = \dot{\vec{r}}(\vec{q})\;.
\end{equation*}
Substituting this into $E$ and reordering the terms we get:
\begin{equation*}
	E = \frac{1}{2}\left( \dot{\vec{q}}^\top M(\vec{q},t) \, \dot{\vec{q}} + \vec{M}_v(\vec{q},t)\cdot \dot{\vec{q}} + M_0(\vec{q},t) \right)\;,
\end{equation*}
where the details are the following:
\begin{align*}
	&M(\vec{q})_{i,j} = \sum_{k} m_k \dfrac{\partial \vec{r}_k}{\partial q_i} \dfrac{\partial \vec{r}_k}{\partial q_j} = \left(J_q^\top M_{eu} \,J_q\right)_{i,j} \longrightarrow \text{\textit{Generalized inertia}}\\[4pt]
	&\vec{M}_v(\vec{q})_i = \sum_{k} m_k \dfrac{\partial \vec{r}_k}{\partial q_i} \dfrac{\partial \vec{r}_k}{\partial t} = \left(J_q^\top M_{eu}\dfrac{\partial \vec{r}}{\partial t}\right) \longrightarrow \text{\textit{Linear velocity coefficient}} \\[4pt]
	&M_0(\vec{q}) = \sum_{k} m_k \left(\dfrac{\partial \vec{r}_k}{\partial t}\right)^2 \longrightarrow \text{\textit{Time dependent potential-like term}}
\end{align*}
which for the systems in which the coordinates transformation is not affected by the time, then we have:
\begin{equation}
	E = \frac{1}{2} \, \dot{\vec{q}}^\top M \, \dot{\vec{q}}\;.
\end{equation}
Clearly if we are dealing with points of the same mass (or a single point) $M=m_{tot}\,I$, with $I$ being the identity matrix.

\section{Hamiltonian Mechanics}

\section{Statistical Mechanics}
\subsection{Canonical Ensemble}
\subsection{Grand Canonical Ensemble}

\section{Fourier Analysis}

\section{Information Theory}
\subsection{Shannon Entropy}

\section{Game Theory}
\subsection{Shapley Value}

\section{Network Theory}

\section{Markov Processes}

\section{Likelihood Function}

\section{P-Value}

\section{Monte Carlo Simulation}


\section{Quantum Mechanics}
Review of the fundamental concepts and result of quantum mechanics. Quantum mechanics is based
on wave functions and operators, so we will introduce the main mathematical tools to describe
them and we will follow by defining the physics that uses them.

\subsection{Hilber Space}
We define Hilbert Space $H$ a space with the following properties:
\begin{itemize}
	\item [1.] $H$ is a vector space
	\item [2.] $H$ has inner product $\langle f , g \rangle$
	\item [3.] Norm $\norm{f} = \sqrt{\langle f , f \rangle}$
	\item [4.] $H$ is complete, so every Cauchy sequence converges in the space $H$.
\end{itemize}
In practice, we will always consider
$H \sim L^2(a,b)$, where $L^2(a,b) = \{ f : \int_{a}^{b} |f(x)|^2 dx < +\infty \} $ such that:
\begin{itemize}
	\item [1.] Vector space
	\item [2.] Inner product $\langle f , g \rangle = \int_{a}^{b} \overline{f(x)}g(x) dx$
	\item [3.] Norm $\norm{f} = \langle f , f \rangle = \int_{a}^{b} |f(x)|^2 dx$
	\item [4.] Complete with respect to the norm $\norm{f}_2 = \sqrt{\int_{a}^{b} |f(x)|^2 dx}$.
\end{itemize}
This is useful for wave functions, since in quantum mechanics the squared wave function
is a probability density function, so the wave functions live in an Hilbert space.

\subsection{Nomenclature}
The concepts expressed in quantum mechanics are mostly the same of linear algebra.
However, since the elements of the vector space are usually functions (wave functions),
they live in an infinite dimensional space. Considering Fourier decomposition, we know
that we can express any function as an infinite sum of waves:
$$
f(x) = \sum_{n=0}^{+\infty} F_n e^{inx} \; .
$$
Here $n$ is an integer that, in some sense, limits the number of allowed wavelengths
according to their spacing. In the limit of the spacing going to zero,
the same function can be expressed as an integral:
$$
f(x) = \int_{-\infty}^{+\infty} F(\nu) e^{i\nu x} d\nu \; ,
$$
with $\nu \in \mathbb{R}$ being the wave frequencies that are now continuous.
As we will see, these two cases represents in quantum mechanics a confined observable
that becomes discretized due to the dependency on the wavelength (like the energy of a particle),
and an observable that is not affected by the quantization (like the position of a particle)
respectively.

With this being said, the generators of the vector space can be either a numberable set of basis
vectors (still infinite, but countable), or an uncountable set of functions
that span the space via integrals. For this reason, a new nomenclature is used
to pay proper attention to this fact, and to simplify the representation
of some recurrent calculations.
\begin{itemize}
	\item \textbf{column vector} : $\vec{v} = \begin{pmatrix}
		v_0 \\
		\vdots \\
		v_N
	\end{pmatrix} \xrightarrow[qm]{} \ket{v}$
	\item \textbf{row conjugate vector} : $\vec{v}^\top = \begin{pmatrix} v_0^*, \dots, v_N^* \end{pmatrix} \xrightarrow[qm]{} \bra{v}$
	\item \textbf{inner product} : $\langle v , u \rangle = \sum_{i=0}^{N}v^*_i\,u_i \xrightarrow[qm]{} \braket{v}{u} = \int_{a}^{b}\overline{v(x)}u(x)dx$
	\item \textbf{matrix} : $A \xrightarrow[qm]{}\hat{A}$. Matrices are also called \textit{operators}, since they are usually applied to functions.
	\item \textbf{Matrix-Vector multiplication} $\vec{u} = A\vec{v} \xrightarrow[qm]{} \ket{u} = \hat{A}\ket{v}$
	\item \textbf{Quadratic form} $\vec{u}^\top A \vec{v} \xrightarrow[qm]{} \bra{u}\hat{A}\ket{v}$. The result is a scalar.
\end{itemize}

Since $L^2$ is a vector space, we can find an orthonormal basis $\{f_n(x)\}$ such that:
\begin{equation}
	f(x) = \sum_{n=0}^{+\infty} F_n \, f_n(x)
\end{equation}
whose coefficients $F_n$ can be found exploiting the orthonormality:
\begin{align*}
	&\braket{f_n(x)}{f_m(x)} = \delta_{mn} \\[2pt]
	&\Rightarrow F_n = \braket{f_n(x)}{f(x)} \; \text{\small{(Fourier method)}}
\end{align*}
where we recall $f_n(x), f(x)$ are vectors in Hilbert space.

\subsection{State Vectors, Basis and Descriptions}
Once $L^2(a,b)$ and Hilbert spaces are defined, we ask what a vector in such space
is. We call the \textit{normalized} vector $\ket{\Psi(t)}\in H$ a
\textbf{quantum state}, whose components depend on a variable $t$ -- usually the
time. All the information about the physics of the system is embedded inside
the quantum state.
The reason why we want it to be normalized is related to \textbf{Born's rule} that
we will see later. 

Now, what is a quantum state $\ket{\Psi}$ exactly? It is an abstract object that lives in Hilbert
space and represents a generic configuration of the particle. We need some basis
of the space that has physical properties so that it can describe the state with
concrete implications (energy, position, momentum, \dots).

In particular, once we fix a vector basis $\{\ket{q_i}\}$ we are able to
rewrite any possible state $\Psi$ as a linear combination of that base --
or superposition in this context, since we are dealing with probability waves. 

The idea is simple: find a basis that represents something physical, then write the
state of the particle according to that physical quantity.
This is possible because we are in $L^2$ and so our states can be seen as wave
functions.

Now we want to define this basis $\{\ket{q_i}\}$. Since we want real quantities,
we know that \textbf{Hermitian} operators have real eigenvalues. This is useful, because
now we can define the Hermitian operator $\hat{Q}$ such that:  
\begin{equation}
    \hat{Q} \ket{q} = q \ket{q}
\end{equation}
where we ask that if a system is in a state $\ket{q}$, then a measurement of the
physical quantity $Q$ will yield the value $q$ with probability 1.
We must be smart to define $\hat{Q}$ such that it actually represents physical quantities
and that they would eventually recover the classical counterpart in the classical limit.

If we manage to define $\hat{Q}$, then to get $\{\ket{q_i}\}$, we can write any state
in Hilber space as:
\begin{equation}
    \ket{\Psi} = \sum_{n=0}^{+\infty}c_n\, \ket{q_n}
\end{equation}

\vspace{16pt}
Here we have to pause for a second to highlight a crucial point about Hilbert space
and basis. A quantum state $\ket{\Psi}$ can be given by a linear combination of
\textit{numerable} vectors $\{\ket{q_i}\}$. However, since we are in $L^2$, we can
also rewrite any vector $\ket{\Psi}$ as a decomposition of functions using continuous
integrals, exactly as Fourier decomposition:
\begin{align*}
    f(x) = \sum_{n = -\infty}^{+\infty}F_n \, e^{inx}\\[2pt]
    f(x) = \int_{-\infty}^{+\infty} F(l) \, e^{ilx} \, dl
\end{align*}
Depending on the physical situation we are in, we might end up in a situation, in
another, or even in both. Some quantities in quantum mechanics depend on the
\textbf{wavelength} of the particle, like the momentum and the energy. Others do not,
like the position of the particle. If we consider a particle in a box, so in a
potential well, it will have a boundary condition $\Psi(0)=\Psi(L)=0$. This condition
means that only some wavelengths are permitted, thus only some energies. The position
is also limited within $0$ and $L$, but in a continuous way. This means that not all
observables have basis that lives in the Hilber space! This doesn't affect that
much in terms of discussion, but it is a profound property of $L^2$ spaces.
The set of eigenvectors of an operator generate the \textbf{spectrum} of the operator.
We will refer to \textbf{discrete spectrum} when the basis is numerable,
and \textbf{continuous spectrum} when our state must be expressed as an integral --
i.e. when the quantity depends on the wavelength or not.
\vspace{16pt}

Now, as we do in physics, we want to predict what could be the outcome of a real
measure of the quantity $Q$. For the discrete spectrum, we now that the state is
given as a superposition of vector basis, each representing a possible outcome
of the measure by definition. So it's reasonable to assume that the projection
of the state $\ket{\Psi}$ on the basis vector $\ket{q_n}$ gives us the
probability amplitude to measure the value $q_n$ when measuring $Q$. From this assumption,
and from the fact that the basis is \textit{orthonormal}, we can obtaine one of the most
important rules of quantum mechanics: \textbf{Born's rule}, which quantifies the
probability of measuring a specific value among the available ones given by
the basis as:
\begin{equation}
    P(Q=q_i) = |\braket{q_i}{\Psi}|^2 = \dfrac{|c_i|^2}{|k|^2} = |c_i|^2
\end{equation}
with $k$ being the length of the state vector, that we asked to be normalized exactly
for this reason.

For the continuous spectrum, the reasoning is the same, but as we did for Fourier
decomposition, at this point we need to work with a continuous function of coefficients
$\{c_n\} \rightarrow \Psi(q)$ such that:
\begin{align*}
    &\Psi(q) = \braket{q}{\Psi} \\
    &\rightarrow \ket{\Psi} = \int_{a}^{b} \Psi(q) \ket{q} \, dq
\end{align*}
and so the probability to measure the quantity $Q$ with value between a range of
values $[q_0, q_1]$ is:
\begin{align*}
    &\rho(q) = |\Psi(q)|^2\\
    &\rightarrow P(q_0 \leq Q \leq q_1) = \int_{q_0}^{q_1} \rho(q) \, dq
\end{align*} 
where $\rho(q)$ is the probability density function to measure a physical
quantity of value $q$.

This is exactly the same thing that happens in Fourier analysis:
we can rewrite any function $f(x)$ as a superposition of different waves with
different amplitudes, which in the limit of the distance between two consecutive
wavelengths going to zero, it becomes an integral over all possible wavelengths.
So we rewrite the function $f(x)$ in terms of a new continuous function $F(\nu)$,
with $\nu$ being a variable in a continuous domain (\textbf{frequency domain}); in quantum
mechanics we rewrite our state $\Psi$ in terms of a new continuous function
$\Psi(q)$, where $q$ is a continuous variable defined in a specific physical space
($q=x \rightarrow$ \textbf{space domain}, $q=p\rightarrow$ \textbf{momentum domain}, \dots).

So to summarize:
\begin{itemize}
	\item A Hermitian operator $\hat{Q}$ is chosen to be associated to a real observable $Q$.
	\item According to the eigenvector basis we use to represent our quantum state $\ket{\Psi}$
	we give a specific description of the state (position, momentum, energy,\dots).
	\item The quantum state $\ket{\Psi}$ can be written as a linear combination of the 
	eigenvectors ${\ket{q_i}}$ of $\hat{Q}$. Measurements of $Q$ yield the eigenvalues 
	associated with these eigenvectors, with some probability to occur.
	\item Born's rule allows to find the probability of measuring specific values of $Q$.
	\item According to the type of observable we are considering (quantized or not),
	Born's rule will get a \textit{probability} ($P(Q=q_i)$) or a
	\textit{probability density} ($\rho(q)$). Continuous spectra allows continuous
	quantities, while discrete spectra allows only the eigenvalues of the eigenvectors
	associated to the quantized observable.
\end{itemize}
All of this is supported by experimental evidences, which is kinda incredible to think that
this actually holds.

\subsection{Define Observable Operators}
The only thing left to do is how to define the operators representing physical quantities.

\subsubsection{Commutator}
We start by the assumption that we can describe any physical observable by finding its
Hermitian operator $\hat{Q}$, whose eigenvalues are the possible results of a measurement.
Then we need to introduce the concept of \textbf{commutator operartor}:
$$
[\hat{A}, \hat{B}] = \hat{A}\hat{B} - \hat{B}\hat{A}
$$
which tells us if we can switch the order of application of two operators.
Now, since we said:
$$
\hat{Q} \ket{q} = q \ket{q}
$$
if two operators commute, they will share the same orthonormal basis of vectors
$\{\ket{q_i}\}$, in fact:
\begin{align*}
&[\hat{A}, \hat{Q}] \ket{q} = (\hat{Q}\hat{A} - \hat{A}\hat{Q}) \ket{q} = 0\\
&\Rightarrow \hat{Q}\hat{A} \ket{q} = \hat{A}\hat{Q} \ket{q} = q\hat{A}\ket{q}\\
&
\end{align*}
which means that $\hat{A}$ must have the same eigenvectors of $\hat{Q}$.
If two operators commute, we usually write the state of the system in terms of
the eigenvalues of both operators for comodity:
$$
\ket{q,a} : \{\hat{Q}\ket{q,a} = q \ket{q,a} \, , \, \hat{A}\ket{q,a} = a \ket{q,a}\}
$$
since they share the same eigenvector basis.
We said that an eigenvalue $q$ is the certain result of a measurement of an
observable $Q$ when the system is exactly in the state $\ket{q}$. This means that for
two commuting operators, we understand that we can measure both the observables $Q$
and $A$ with certainty if the system is in the state $\ket{q,a}$.

If the two operators do not commute, so if they have a non-zero commutator
$[\hat{A}, \hat{B}] \neq 0$, then an important property arises:
the \textbf{uncertainty principle}.
This means that it doesn't exist a state $\ket{\Psi}$ in which we can measure at the
same time the two observables $Q$ and $A$ with certainty, since they have different
eigenvectors.


TODO: Capire come cazzo definire gli operatori e scrivere i principali

\subsection{Schrödinger Equation}
Schrödinger equation is exactly the equivalent of Newwtons's second law of Dynamics.
It comes from empirical observations regarding both the wave-like probabilistic behavior of quantum particles, and de Broglie particle wavelength assumption -- in addition to the already known photon's wavelength.

The idea is to define a for a single particle a function $\Psi(x,t)$ that 
For a single non-relativistic particle of mass $m$, moving in a potential $V(\mathbf{r},t)$, the time-dependent Schrödinger equation is given by:
\begin{equation*}
i \hbar \dfrac{\partial \Psi}{\partial t}(x,t) = \left[-\dfrac{\hbar^2}{2m} \nabla^2 + V(x,t) \right] \Psi(x,t)
\end{equation*}
which if we consider a steady state -- thus we remove the time dependency -- we get:
\begin{equation*}
-\dfrac{\hbar^2}{2m} \nabla^2 \psi(x) + V(x) \psi(x) = E \psi(x)
\end{equation*}
where we decomposed the wave function in two pieces $\Psi(x,t) = \psi(x) \phi(t)$.

Since we are assuming that the wave function is a probability density function, it should be normalized in $L^2$ space:
\begin{equation*}
    \int_{-\infty}^{+\infty} |\Psi(x,t)|^2 \, dx = 1
\end{equation*}

In Dirac notation we rewrite all the relations above:
\begin{align}
&i \hbar \dfrac{\partial}{\partial t} \ket{\Psi(t)} = \hat{H}(t) \ket{\Psi(t)} \longrightarrow \text{\small Schrödinger Equation} \\[2pt]
&\hat{H} = \left[-\dfrac{\hbar^2}{2m} \nabla^2 + V(x)\right] \longrightarrow \text{\small Hamiltonian Operator} \\[2pt]
&\braket{\Psi(t)}{\Psi(t)} = 1 \longrightarrow \text{\small Normalization Condition}
\end{align}
Dirac notation is useful to express quantum states as elements of Hilbert space, without having to refer to a specific basis. These are just 
\end{document}
